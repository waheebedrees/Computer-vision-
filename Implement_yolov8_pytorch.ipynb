{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "T_ktHPjW9o8c",
        "D3MYfqjR9ncA"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyODqo0SxrR2FgK2dKndO+nG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waheebedrees/Computer-vision-/blob/main/Implement_yolov8_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import math, os, random, cv2, numpy, torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "cucPKh7w8H4A"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# utils"
      ],
      "metadata": {
        "id": "T_ktHPjW9o8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import random\n",
        "from time import time\n",
        "\n",
        "import math\n",
        "import numpy\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.nn.functional import cross_entropy\n",
        "\n",
        "\n",
        "def setup_seed():\n",
        "    \"\"\"\n",
        "    Setup random seed.\n",
        "    \"\"\"\n",
        "    random.seed(0)\n",
        "    numpy.random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "def setup_multi_processes():\n",
        "    \"\"\"\n",
        "    Setup multi-processing environment variables.\n",
        "    \"\"\"\n",
        "    import cv2\n",
        "    from os import environ\n",
        "    from platform import system\n",
        "\n",
        "    # set multiprocess start method as `fork` to speed up the training\n",
        "    if system() != 'Windows':\n",
        "        torch.multiprocessing.set_start_method('fork', force=True)\n",
        "\n",
        "    # disable opencv multithreading to avoid system being overloaded\n",
        "    cv2.setNumThreads(0)\n",
        "\n",
        "    # setup OMP threads\n",
        "    if 'OMP_NUM_THREADS' not in environ:\n",
        "        environ['OMP_NUM_THREADS'] = '1'\n",
        "\n",
        "    # setup MKL threads\n",
        "    if 'MKL_NUM_THREADS' not in environ:\n",
        "        environ['MKL_NUM_THREADS'] = '1'\n",
        "\n",
        "\n",
        "def export_onnx(args):\n",
        "    import onnx  # noqa\n",
        "\n",
        "    inputs = ['images']\n",
        "    outputs = ['outputs']\n",
        "    dynamic = {'outputs': {0: 'batch', 1: 'anchors'}}\n",
        "\n",
        "    m = torch.load('./weights/best.pt')['model'].float()\n",
        "    x = torch.zeros((1, 3, args.input_size, args.input_size))\n",
        "\n",
        "    torch.onnx.export(m.cpu(), x.cpu(),\n",
        "                      f='./weights/best.onnx',\n",
        "                      verbose=False,\n",
        "                      opset_version=12,\n",
        "                      # WARNING: DNN inference with torch>=1.12 may require do_constant_folding=False\n",
        "                      do_constant_folding=True,\n",
        "                      input_names=inputs,\n",
        "                      output_names=outputs,\n",
        "                      dynamic_axes=dynamic or None)\n",
        "\n",
        "    # Checks\n",
        "    model_onnx = onnx.load('./weights/best.onnx')  # load onnx model\n",
        "    onnx.checker.check_model(model_onnx)  # check onnx model\n",
        "\n",
        "    onnx.save(model_onnx, './weights/best.onnx')\n",
        "    # Inference example\n",
        "    # https://github.com/ultralytics/ultralytics/blob/main/ultralytics/nn/autobackend.py\n",
        "\n",
        "\n",
        "def wh2xy(x):\n",
        "    y = x.clone() if isinstance(x, torch.Tensor) else numpy.copy(x)\n",
        "    y[:, 0] = x[:, 0] - x[:, 2] / 2  # top left x\n",
        "    y[:, 1] = x[:, 1] - x[:, 3] / 2  # top left y\n",
        "    y[:, 2] = x[:, 0] + x[:, 2] / 2  # bottom right x\n",
        "    y[:, 3] = x[:, 1] + x[:, 3] / 2  # bottom right y\n",
        "    return y\n",
        "\n",
        "\n",
        "def make_anchors(x, strides, offset=0.5):\n",
        "    assert x is not None\n",
        "    anchor_tensor, stride_tensor = [], []\n",
        "    dtype, device = x[0].dtype, x[0].device\n",
        "    for i, stride in enumerate(strides):\n",
        "        _, _, h, w = x[i].shape\n",
        "        sx = torch.arange(end=w, device=device, dtype=dtype) + offset  # shift x\n",
        "        sy = torch.arange(end=h, device=device, dtype=dtype) + offset  # shift y\n",
        "        sy, sx = torch.meshgrid(sy, sx)\n",
        "        anchor_tensor.append(torch.stack((sx, sy), -1).view(-1, 2))\n",
        "        stride_tensor.append(torch.full((h * w, 1), stride, dtype=dtype, device=device))\n",
        "    return torch.cat(anchor_tensor), torch.cat(stride_tensor)\n",
        "\n",
        "\n",
        "def compute_metric(output, target, iou_v):\n",
        "    # intersection(N,M) = (rb(N,M,2) - lt(N,M,2)).clamp(0).prod(2)\n",
        "    (a1, a2) = target[:, 1:].unsqueeze(1).chunk(2, 2)\n",
        "    (b1, b2) = output[:, :4].unsqueeze(0).chunk(2, 2)\n",
        "    intersection = (torch.min(a2, b2) - torch.max(a1, b1)).clamp(0).prod(2)\n",
        "    # IoU = intersection / (area1 + area2 - intersection)\n",
        "    iou = intersection / ((a2 - a1).prod(2) + (b2 - b1).prod(2) - intersection + 1e-7)\n",
        "\n",
        "    correct = numpy.zeros((output.shape[0], iou_v.shape[0]))\n",
        "    correct = correct.astype(bool)\n",
        "    for i in range(len(iou_v)):\n",
        "        # IoU > threshold and classes match\n",
        "        x = torch.where((iou >= iou_v[i]) & (target[:, 0:1] == output[:, 5]))\n",
        "        if x[0].shape[0]:\n",
        "            matches = torch.cat((torch.stack(x, 1),\n",
        "                                 iou[x[0], x[1]][:, None]), 1).cpu().numpy()  # [label, detect, iou]\n",
        "            if x[0].shape[0] > 1:\n",
        "                matches = matches[matches[:, 2].argsort()[::-1]]\n",
        "                matches = matches[numpy.unique(matches[:, 1], return_index=True)[1]]\n",
        "                matches = matches[numpy.unique(matches[:, 0], return_index=True)[1]]\n",
        "            correct[matches[:, 1].astype(int), i] = True\n",
        "    return torch.tensor(correct, dtype=torch.bool, device=output.device)\n",
        "\n",
        "\n",
        "def non_max_suppression(outputs, confidence_threshold=0.001, iou_threshold=0.7):\n",
        "    max_wh = 7680\n",
        "    max_det = 300\n",
        "    max_nms = 30000\n",
        "\n",
        "    bs = outputs.shape[0]  # batch size\n",
        "    nc = outputs.shape[1] - 4  # number of classes\n",
        "    xc = outputs[:, 4:4 + nc].amax(1) > confidence_threshold  # candidates\n",
        "\n",
        "    # Settings\n",
        "    start = time()\n",
        "    limit = 0.5 + 0.05 * bs  # seconds to quit after\n",
        "    output = [torch.zeros((0, 6), device=outputs.device)] * bs\n",
        "    for index, x in enumerate(outputs):  # image index, image inference\n",
        "        x = x.transpose(0, -1)[xc[index]]  # confidence\n",
        "\n",
        "        # If none remain process next image\n",
        "        if not x.shape[0]:\n",
        "            continue\n",
        "\n",
        "        # matrix nx6 (box, confidence, cls)\n",
        "        box, cls = x.split((4, nc), 1)\n",
        "        box = wh2xy(box)  # (cx, cy, w, h) to (x1, y1, x2, y2)\n",
        "        if nc > 1:\n",
        "            i, j = (cls > confidence_threshold).nonzero(as_tuple=False).T\n",
        "            x = torch.cat((box[i], x[i, 4 + j, None], j[:, None].float()), 1)\n",
        "        else:  # best class only\n",
        "            conf, j = cls.max(1, keepdim=True)\n",
        "            x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > confidence_threshold]\n",
        "\n",
        "        # Check shape\n",
        "        n = x.shape[0]  # number of boxes\n",
        "        if not n:  # no boxes\n",
        "            continue\n",
        "        x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence and remove excess boxes\n",
        "\n",
        "        # Batched NMS\n",
        "        c = x[:, 5:6] * max_wh  # classes\n",
        "        boxes, scores = x[:, :4] + c, x[:, 4]  # boxes, scores\n",
        "        indices = torchvision.ops.nms(boxes, scores, iou_threshold)  # NMS\n",
        "        indices = indices[:max_det]  # limit detections\n",
        "\n",
        "        output[index] = x[indices]\n",
        "        if (time() - start) > limit:\n",
        "            break  # time limit exceeded\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "def smooth(y, f=0.05):\n",
        "    # Box filter of fraction f\n",
        "    nf = round(len(y) * f * 2) // 2 + 1  # number of filter elements (must be odd)\n",
        "    p = numpy.ones(nf // 2)  # ones padding\n",
        "    yp = numpy.concatenate((p * y[0], y, p * y[-1]), 0)  # y padded\n",
        "    return numpy.convolve(yp, numpy.ones(nf) / nf, mode='valid')  # y-smoothed\n",
        "\n",
        "\n",
        "def compute_ap(tp, conf, pred_cls, target_cls, eps=1e-16):\n",
        "    \"\"\"\n",
        "    Compute the average precision, given the recall and precision curves.\n",
        "    Source: https://github.com/rafaelpadilla/Object-Detection-Metrics.\n",
        "    # Arguments\n",
        "        tp:  True positives (nparray, nx1 or nx10).\n",
        "        conf:  Object-ness value from 0-1 (nparray).\n",
        "        pred_cls:  Predicted object classes (nparray).\n",
        "        target_cls:  True object classes (nparray).\n",
        "    # Returns\n",
        "        The average precision\n",
        "    \"\"\"\n",
        "    # Sort by object-ness\n",
        "    i = numpy.argsort(-conf)\n",
        "    tp, conf, pred_cls = tp[i], conf[i], pred_cls[i]\n",
        "\n",
        "    # Find unique classes\n",
        "    unique_classes, nt = numpy.unique(target_cls, return_counts=True)\n",
        "    nc = unique_classes.shape[0]  # number of classes, number of detections\n",
        "\n",
        "    # Create Precision-Recall curve and compute AP for each class\n",
        "    p = numpy.zeros((nc, 1000))\n",
        "    r = numpy.zeros((nc, 1000))\n",
        "    ap = numpy.zeros((nc, tp.shape[1]))\n",
        "    px, py = numpy.linspace(0, 1, 1000), []  # for plotting\n",
        "    for ci, c in enumerate(unique_classes):\n",
        "        i = pred_cls == c\n",
        "        nl = nt[ci]  # number of labels\n",
        "        no = i.sum()  # number of outputs\n",
        "        if no == 0 or nl == 0:\n",
        "            continue\n",
        "\n",
        "        # Accumulate FPs and TPs\n",
        "        fpc = (1 - tp[i]).cumsum(0)\n",
        "        tpc = tp[i].cumsum(0)\n",
        "\n",
        "        # Recall\n",
        "        recall = tpc / (nl + eps)  # recall curve\n",
        "        # negative x, xp because xp decreases\n",
        "        r[ci] = numpy.interp(-px, -conf[i], recall[:, 0], left=0)\n",
        "\n",
        "        # Precision\n",
        "        precision = tpc / (tpc + fpc)  # precision curve\n",
        "        p[ci] = numpy.interp(-px, -conf[i], precision[:, 0], left=1)  # p at pr_score\n",
        "\n",
        "        # AP from recall-precision curve\n",
        "        for j in range(tp.shape[1]):\n",
        "            m_rec = numpy.concatenate(([0.0], recall[:, j], [1.0]))\n",
        "            m_pre = numpy.concatenate(([1.0], precision[:, j], [0.0]))\n",
        "\n",
        "            # Compute the precision envelope\n",
        "            m_pre = numpy.flip(numpy.maximum.accumulate(numpy.flip(m_pre)))\n",
        "\n",
        "            # Integrate area under curve\n",
        "            x = numpy.linspace(0, 1, 101)  # 101-point interp (COCO)\n",
        "            ap[ci, j] = numpy.trapz(numpy.interp(x, m_rec, m_pre), x)  # integrate\n",
        "\n",
        "    # Compute F1 (harmonic mean of precision and recall)\n",
        "    f1 = 2 * p * r / (p + r + eps)\n",
        "\n",
        "    i = smooth(f1.mean(0), 0.1).argmax()  # max F1 index\n",
        "    p, r, f1 = p[:, i], r[:, i], f1[:, i]\n",
        "    tp = (r * nt).round()  # true positives\n",
        "    fp = (tp / (p + eps) - tp).round()  # false positives\n",
        "    ap50, ap = ap[:, 0], ap.mean(1)  # AP@0.5, AP@0.5:0.95\n",
        "    m_pre, m_rec = p.mean(), r.mean()\n",
        "    map50, mean_ap = ap50.mean(), ap.mean()\n",
        "    return tp, fp, m_pre, m_rec, map50, mean_ap\n",
        "\n",
        "\n",
        "def compute_iou(box1, box2, eps=1e-7):\n",
        "    # Returns Intersection over Union (IoU) of box1(1,4) to box2(n,4)\n",
        "\n",
        "    # Get the coordinates of bounding boxes\n",
        "    b1_x1, b1_y1, b1_x2, b1_y2 = box1.chunk(4, -1)\n",
        "    b2_x1, b2_y1, b2_x2, b2_y2 = box2.chunk(4, -1)\n",
        "    w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1 + eps\n",
        "    w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1 + eps\n",
        "\n",
        "    # Intersection area\n",
        "    inter = (b1_x2.minimum(b2_x2) - b1_x1.maximum(b2_x1)).clamp(0) * \\\n",
        "            (b1_y2.minimum(b2_y2) - b1_y1.maximum(b2_y1)).clamp(0)\n",
        "\n",
        "    # Union Area\n",
        "    union = w1 * h1 + w2 * h2 - inter + eps\n",
        "\n",
        "    # IoU\n",
        "    iou = inter / union\n",
        "    cw = b1_x2.maximum(b2_x2) - b1_x1.minimum(b2_x1)  # convex (smallest enclosing box) width\n",
        "    ch = b1_y2.maximum(b2_y2) - b1_y1.minimum(b2_y1)  # convex height\n",
        "    c2 = cw ** 2 + ch ** 2 + eps  # convex diagonal squared\n",
        "    rho2 = ((b2_x1 + b2_x2 - b1_x1 - b1_x2) ** 2 + (b2_y1 + b2_y2 - b1_y1 - b1_y2) ** 2) / 4  # center dist ** 2\n",
        "    # https://github.com/Zzh-tju/DIoU-SSD-pytorch/blob/master/utils/box/box_utils.py#L47\n",
        "    v = (4 / math.pi ** 2) * (torch.atan(w2 / h2) - torch.atan(w1 / h1)).pow(2)\n",
        "    with torch.no_grad():\n",
        "        alpha = v / (v - iou + (1 + eps))\n",
        "    return iou - (rho2 / c2 + v * alpha)  # CIoU\n",
        "\n",
        "\n",
        "def strip_optimizer(filename):\n",
        "    x = torch.load(filename, map_location=\"cpu\")\n",
        "    x['model'].half()  # to FP16\n",
        "    for p in x['model'].parameters():\n",
        "        p.requires_grad = False\n",
        "    torch.save(x, f=filename)\n",
        "\n",
        "\n",
        "def clip_gradients(model, max_norm=10.0):\n",
        "    parameters = model.parameters()\n",
        "    torch.nn.utils.clip_grad_norm_(parameters, max_norm=max_norm)\n",
        "\n",
        "\n",
        "def load_weight(model, ckpt):\n",
        "    dst = model.state_dict()\n",
        "    src = torch.load(ckpt)['model'].float().cpu()\n",
        "\n",
        "    ckpt = {}\n",
        "    for k, v in src.state_dict().items():\n",
        "        if k in dst and v.shape == dst[k].shape:\n",
        "            ckpt[k] = v\n",
        "\n",
        "    model.load_state_dict(state_dict=ckpt, strict=False)\n",
        "    return model\n",
        "\n",
        "\n",
        "def set_params(model, decay):\n",
        "    p1 = []\n",
        "    p2 = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if not param.requires_grad:\n",
        "            continue\n",
        "        if param.ndim <= 1 or name.endswith(\".bias\"):\n",
        "            p1.append(param)\n",
        "        else:\n",
        "            p2.append(param)\n",
        "    return [{'params': p1, 'weight_decay': 0.00},\n",
        "            {'params': p2, 'weight_decay': decay}]\n",
        "\n",
        "\n",
        "def plot_lr(args, optimizer, scheduler, num_steps):\n",
        "    from matplotlib import pyplot\n",
        "\n",
        "    optimizer = copy.copy(optimizer)\n",
        "    scheduler = copy.copy(scheduler)\n",
        "\n",
        "    y = []\n",
        "    for epoch in range(args.epochs):\n",
        "        for i in range(num_steps):\n",
        "            step = i + num_steps * epoch\n",
        "            scheduler.step(step, optimizer)\n",
        "            y.append(optimizer.param_groups[0]['lr'])\n",
        "    print(y[0])\n",
        "    print(y[-1])\n",
        "    pyplot.plot(y, '.-', label='LR')\n",
        "    pyplot.xlabel('step')\n",
        "    pyplot.ylabel('LR')\n",
        "    pyplot.grid()\n",
        "    pyplot.xlim(0, args.epochs * num_steps)\n",
        "    pyplot.ylim(0)\n",
        "    pyplot.savefig('./weights/lr.png', dpi=200)\n",
        "    pyplot.close()\n",
        "\n",
        "\n",
        "class CosineLR:\n",
        "    def __init__(self, args, params, num_steps):\n",
        "        max_lr = params['max_lr']\n",
        "        min_lr = params['min_lr']\n",
        "\n",
        "        warmup_steps = int(max(params['warmup_epochs'] * num_steps, 100))\n",
        "        decay_steps = int(args.epochs * num_steps - warmup_steps)\n",
        "\n",
        "        warmup_lr = numpy.linspace(min_lr, max_lr, int(warmup_steps))\n",
        "\n",
        "        decay_lr = []\n",
        "        for step in range(1, decay_steps + 1):\n",
        "            alpha = math.cos(math.pi * step / decay_steps)\n",
        "            decay_lr.append(min_lr + 0.5 * (max_lr - min_lr) * (1 + alpha))\n",
        "\n",
        "        self.total_lr = numpy.concatenate((warmup_lr, decay_lr))\n",
        "\n",
        "    def step(self, step, optimizer):\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = self.total_lr[step]\n",
        "\n",
        "\n",
        "class LinearLR:\n",
        "    def __init__(self, args, params, num_steps):\n",
        "        max_lr = params['max_lr']\n",
        "        min_lr = params['min_lr']\n",
        "\n",
        "        warmup_steps = int(max(params['warmup_epochs'] * num_steps, 100))\n",
        "        decay_steps = int(args.epochs * num_steps - warmup_steps)\n",
        "\n",
        "        warmup_lr = numpy.linspace(min_lr, max_lr, int(warmup_steps), endpoint=False)\n",
        "        decay_lr = numpy.linspace(max_lr, min_lr, decay_steps)\n",
        "\n",
        "        self.total_lr = numpy.concatenate((warmup_lr, decay_lr))\n",
        "\n",
        "    def step(self, step, optimizer):\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = self.total_lr[step]\n",
        "\n",
        "\n",
        "class EMA:\n",
        "    \"\"\"\n",
        "    Updated Exponential Moving Average (EMA) from https://github.com/rwightman/pytorch-image-models\n",
        "    Keeps a moving average of everything in the model state_dict (parameters and buffers)\n",
        "    For EMA details see https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, decay=0.9999, tau=2000, updates=0):\n",
        "        # Create EMA\n",
        "        self.ema = copy.deepcopy(model).eval()  # FP32 EMA\n",
        "        self.updates = updates  # number of EMA updates\n",
        "        # decay exponential ramp (to help early epochs)\n",
        "        self.decay = lambda x: decay * (1 - math.exp(-x / tau))\n",
        "        for p in self.ema.parameters():\n",
        "            p.requires_grad_(False)\n",
        "\n",
        "    def update(self, model):\n",
        "        if hasattr(model, 'module'):\n",
        "            model = model.module\n",
        "        # Update EMA parameters\n",
        "        with torch.no_grad():\n",
        "            self.updates += 1\n",
        "            d = self.decay(self.updates)\n",
        "\n",
        "            msd = model.state_dict()  # model state_dict\n",
        "            for k, v in self.ema.state_dict().items():\n",
        "                if v.dtype.is_floating_point:\n",
        "                    v *= d\n",
        "                    v += (1 - d) * msd[k].detach()\n",
        "\n",
        "\n",
        "class AverageMeter:\n",
        "    def __init__(self):\n",
        "        self.num = 0\n",
        "        self.sum = 0\n",
        "        self.avg = 0\n",
        "\n",
        "    def update(self, v, n):\n",
        "        if not math.isnan(float(v)):\n",
        "            self.num = self.num + n\n",
        "            self.sum = self.sum + v * n\n",
        "            self.avg = self.sum / self.num\n",
        "\n",
        "\n",
        "class Assigner(torch.nn.Module):\n",
        "    def __init__(self, nc=80, top_k=13, alpha=1.0, beta=6.0, eps=1E-9):\n",
        "        super().__init__()\n",
        "        self.top_k = top_k\n",
        "        self.nc = nc\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.eps = eps\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, pd_scores, pd_bboxes, anc_points, gt_labels, gt_bboxes, mask_gt):\n",
        "        batch_size = pd_scores.size(0)\n",
        "        num_max_boxes = gt_bboxes.size(1)\n",
        "\n",
        "        if num_max_boxes == 0:\n",
        "            device = gt_bboxes.device\n",
        "            return (torch.zeros_like(pd_bboxes).to(device),\n",
        "                    torch.zeros_like(pd_scores).to(device),\n",
        "                    torch.zeros_like(pd_scores[..., 0]).to(device))\n",
        "\n",
        "        num_anchors = anc_points.shape[0]\n",
        "        shape = gt_bboxes.shape\n",
        "        lt, rb = gt_bboxes.view(-1, 1, 4).chunk(2, 2)\n",
        "        mask_in_gts = torch.cat((anc_points[None] - lt, rb - anc_points[None]), dim=2)\n",
        "        mask_in_gts = mask_in_gts.view(shape[0], shape[1], num_anchors, -1).amin(3).gt_(self.eps)\n",
        "        na = pd_bboxes.shape[-2]\n",
        "        gt_mask = (mask_in_gts * mask_gt).bool()  # b, max_num_obj, h*w\n",
        "        overlaps = torch.zeros([batch_size, num_max_boxes, na], dtype=pd_bboxes.dtype, device=pd_bboxes.device)\n",
        "        bbox_scores = torch.zeros([batch_size, num_max_boxes, na], dtype=pd_scores.dtype, device=pd_scores.device)\n",
        "\n",
        "        ind = torch.zeros([2, batch_size, num_max_boxes], dtype=torch.long)  # 2, b, max_num_obj\n",
        "        ind[0] = torch.arange(end=batch_size).view(-1, 1).expand(-1, num_max_boxes)  # b, max_num_obj\n",
        "        ind[1] = gt_labels.squeeze(-1)  # b, max_num_obj\n",
        "        bbox_scores[gt_mask] = pd_scores[ind[0], :, ind[1]][gt_mask]  # b, max_num_obj, h*w\n",
        "\n",
        "        pd_boxes = pd_bboxes.unsqueeze(1).expand(-1, num_max_boxes, -1, -1)[gt_mask]\n",
        "        gt_boxes = gt_bboxes.unsqueeze(2).expand(-1, -1, na, -1)[gt_mask]\n",
        "        overlaps[gt_mask] = compute_iou(gt_boxes, pd_boxes).squeeze(-1).clamp_(0)\n",
        "\n",
        "        align_metric = bbox_scores.pow(self.alpha) * overlaps.pow(self.beta)\n",
        "\n",
        "        top_k_mask = mask_gt.expand(-1, -1, self.top_k).bool()\n",
        "        top_k_metrics, top_k_indices = torch.topk(align_metric, self.top_k, dim=-1, largest=True)\n",
        "        if top_k_mask is None:\n",
        "            top_k_mask = (top_k_metrics.max(-1, keepdim=True)[0] > self.eps).expand_as(top_k_indices)\n",
        "        top_k_indices.masked_fill_(~top_k_mask, 0)\n",
        "\n",
        "        mask_top_k = torch.zeros(align_metric.shape, dtype=torch.int8, device=top_k_indices.device)\n",
        "        ones = torch.ones_like(top_k_indices[:, :, :1], dtype=torch.int8, device=top_k_indices.device)\n",
        "        for k in range(self.top_k):\n",
        "            mask_top_k.scatter_add_(-1, top_k_indices[:, :, k:k + 1], ones)\n",
        "        mask_top_k.masked_fill_(mask_top_k > 1, 0)\n",
        "        mask_top_k = mask_top_k.to(align_metric.dtype)\n",
        "        mask_pos = mask_top_k * mask_in_gts * mask_gt\n",
        "\n",
        "        fg_mask = mask_pos.sum(-2)\n",
        "        if fg_mask.max() > 1:\n",
        "            mask_multi_gts = (fg_mask.unsqueeze(1) > 1).expand(-1, num_max_boxes, -1)\n",
        "            max_overlaps_idx = overlaps.argmax(1)\n",
        "\n",
        "            is_max_overlaps = torch.zeros(mask_pos.shape, dtype=mask_pos.dtype, device=mask_pos.device)\n",
        "            is_max_overlaps.scatter_(1, max_overlaps_idx.unsqueeze(1), 1)\n",
        "\n",
        "            mask_pos = torch.where(mask_multi_gts, is_max_overlaps, mask_pos).float()\n",
        "            fg_mask = mask_pos.sum(-2)\n",
        "        target_gt_idx = mask_pos.argmax(-2)\n",
        "\n",
        "        # Assigned target\n",
        "        index = torch.arange(end=batch_size, dtype=torch.int64, device=gt_labels.device)[..., None]\n",
        "        target_index = target_gt_idx + index * num_max_boxes\n",
        "        target_labels = gt_labels.long().flatten()[target_index]\n",
        "\n",
        "        target_bboxes = gt_bboxes.view(-1, gt_bboxes.shape[-1])[target_index]\n",
        "\n",
        "        # Assigned target scores\n",
        "        target_labels.clamp_(0)\n",
        "\n",
        "        target_scores = torch.zeros((target_labels.shape[0], target_labels.shape[1], self.nc),\n",
        "                                    dtype=torch.int64,\n",
        "                                    device=target_labels.device)\n",
        "        target_scores.scatter_(2, target_labels.unsqueeze(-1), 1)\n",
        "\n",
        "        fg_scores_mask = fg_mask[:, :, None].repeat(1, 1, self.nc)\n",
        "        target_scores = torch.where(fg_scores_mask > 0, target_scores, 0)\n",
        "\n",
        "        # Normalize\n",
        "        align_metric *= mask_pos\n",
        "        pos_align_metrics = align_metric.amax(dim=-1, keepdim=True)\n",
        "        pos_overlaps = (overlaps * mask_pos).amax(dim=-1, keepdim=True)\n",
        "        norm_align_metric = (align_metric * pos_overlaps / (pos_align_metrics + self.eps)).amax(-2).unsqueeze(-1)\n",
        "        target_scores = target_scores * norm_align_metric\n",
        "\n",
        "        return target_bboxes, target_scores, fg_mask.bool()\n",
        "\n",
        "\n",
        "class QFL(torch.nn.Module):\n",
        "    def __init__(self, beta=2.0):\n",
        "        super().__init__()\n",
        "        self.beta = beta\n",
        "        self.bce_loss = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        bce_loss = self.bce_loss(outputs, targets)\n",
        "        return torch.pow(torch.abs(targets - outputs.sigmoid()), self.beta) * bce_loss\n",
        "\n",
        "\n",
        "class VFL(torch.nn.Module):\n",
        "    def __init__(self, alpha=0.75, gamma=2.00, iou_weighted=True):\n",
        "        super().__init__()\n",
        "        assert alpha >= 0.0\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.iou_weighted = iou_weighted\n",
        "        self.bce_loss = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        assert outputs.size() == targets.size()\n",
        "        targets = targets.type_as(outputs)\n",
        "\n",
        "        if self.iou_weighted:\n",
        "            focal_weight = targets * (targets > 0.0).float() + \\\n",
        "                           self.alpha * (outputs.sigmoid() - targets).abs().pow(self.gamma) * \\\n",
        "                           (targets <= 0.0).float()\n",
        "\n",
        "        else:\n",
        "            focal_weight = (targets > 0.0).float() + \\\n",
        "                           self.alpha * (outputs.sigmoid() - targets).abs().pow(self.gamma) * \\\n",
        "                           (targets <= 0.0).float()\n",
        "\n",
        "        return self.bce_loss(outputs, targets) * focal_weight\n",
        "\n",
        "\n",
        "class BoxLoss(torch.nn.Module):\n",
        "    def __init__(self, dfl_ch):\n",
        "        super().__init__()\n",
        "        self.dfl_ch = dfl_ch\n",
        "\n",
        "    def forward(self, pred_dist, pred_bboxes, anchor_points, target_bboxes, target_scores, target_scores_sum, fg_mask):\n",
        "        # IoU loss\n",
        "        weight = torch.masked_select(target_scores.sum(-1), fg_mask).unsqueeze(-1)\n",
        "        iou = compute_iou(pred_bboxes[fg_mask], target_bboxes[fg_mask])\n",
        "        loss_box = ((1.0 - iou) * weight).sum() / target_scores_sum\n",
        "\n",
        "        # DFL loss\n",
        "        a, b = target_bboxes.chunk(2, -1)\n",
        "        target = torch.cat((anchor_points - a, b - anchor_points), -1)\n",
        "        target = target.clamp(0, self.dfl_ch - 0.01)\n",
        "        loss_dfl = self.df_loss(pred_dist[fg_mask].view(-1, self.dfl_ch + 1), target[fg_mask])\n",
        "        loss_dfl = (loss_dfl * weight).sum() / target_scores_sum\n",
        "\n",
        "        return loss_box, loss_dfl\n",
        "\n",
        "    @staticmethod\n",
        "    def df_loss(pred_dist, target):\n",
        "        # Distribution Focal Loss (DFL)\n",
        "        # https://ieeexplore.ieee.org/document/9792391\n",
        "        tl = target.long()  # target left\n",
        "        tr = tl + 1  # target right\n",
        "        wl = tr - target  # weight left\n",
        "        wr = 1 - wl  # weight right\n",
        "        left_loss = cross_entropy(pred_dist, tl.view(-1), reduction='none').view(tl.shape)\n",
        "        right_loss = cross_entropy(pred_dist, tr.view(-1), reduction='none').view(tl.shape)\n",
        "        return (left_loss * wl + right_loss * wr).mean(-1, keepdim=True)\n",
        "\n",
        "\n",
        "class ComputeLoss:\n",
        "    def __init__(self, model, params):\n",
        "        if hasattr(model, 'module'):\n",
        "            model = model.module\n",
        "\n",
        "        device = next(model.parameters()).device\n",
        "\n",
        "        m = model.head  # Head() module\n",
        "\n",
        "        self.params = params\n",
        "        self.stride = m.stride\n",
        "        self.nc = m.nc\n",
        "        self.no = m.no\n",
        "        self.reg_max = m.ch\n",
        "        self.device = device\n",
        "\n",
        "        self.box_loss = BoxLoss(m.ch - 1).to(device)\n",
        "        self.cls_loss = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
        "        self.assigner = Assigner(nc=self.nc, top_k=10, alpha=0.5, beta=6.0)\n",
        "\n",
        "        self.project = torch.arange(m.ch, dtype=torch.float, device=device)\n",
        "\n",
        "    def box_decode(self, anchor_points, pred_dist):\n",
        "        b, a, c = pred_dist.shape\n",
        "        pred_dist = pred_dist.view(b, a, 4, c // 4)\n",
        "        pred_dist = pred_dist.softmax(3)\n",
        "        pred_dist = pred_dist.matmul(self.project.type(pred_dist.dtype))\n",
        "        lt, rb = pred_dist.chunk(2, -1)\n",
        "        x1y1 = anchor_points - lt\n",
        "        x2y2 = anchor_points + rb\n",
        "        return torch.cat(tensors=(x1y1, x2y2), dim=-1)\n",
        "\n",
        "    def __call__(self, outputs, targets):\n",
        "        x = torch.cat([i.view(outputs[0].shape[0], self.no, -1) for i in outputs], dim=2)\n",
        "        pred_distri, pred_scores = x.split(split_size=(self.reg_max * 4, self.nc), dim=1)\n",
        "\n",
        "        pred_scores = pred_scores.permute(0, 2, 1).contiguous()\n",
        "        pred_distri = pred_distri.permute(0, 2, 1).contiguous()\n",
        "\n",
        "        data_type = pred_scores.dtype\n",
        "        batch_size = pred_scores.shape[0]\n",
        "        input_size = torch.tensor(outputs[0].shape[2:], device=self.device, dtype=data_type) * self.stride[0]\n",
        "        anchor_points, stride_tensor = make_anchors(outputs, self.stride, offset=0.5)\n",
        "\n",
        "        idx = targets['idx'].view(-1, 1)\n",
        "        cls = targets['cls'].view(-1, 1)\n",
        "        box = targets['box']\n",
        "\n",
        "        targets = torch.cat((idx, cls, box), dim=1).to(self.device)\n",
        "        if targets.shape[0] == 0:\n",
        "            gt = torch.zeros(batch_size, 0, 5, device=self.device)\n",
        "        else:\n",
        "            i = targets[:, 0]\n",
        "            _, counts = i.unique(return_counts=True)\n",
        "            counts = counts.to(dtype=torch.int32)\n",
        "            gt = torch.zeros(batch_size, counts.max(), 5, device=self.device)\n",
        "            for j in range(batch_size):\n",
        "                matches = i == j\n",
        "                n = matches.sum()\n",
        "                if n:\n",
        "                    gt[j, :n] = targets[matches, 1:]\n",
        "            x = gt[..., 1:5].mul_(input_size[[1, 0, 1, 0]])\n",
        "            y = torch.empty_like(x)\n",
        "            dw = x[..., 2] / 2  # half-width\n",
        "            dh = x[..., 3] / 2  # half-height\n",
        "            y[..., 0] = x[..., 0] - dw  # top left x\n",
        "            y[..., 1] = x[..., 1] - dh  # top left y\n",
        "            y[..., 2] = x[..., 0] + dw  # bottom right x\n",
        "            y[..., 3] = x[..., 1] + dh  # bottom right y\n",
        "            gt[..., 1:5] = y\n",
        "        gt_labels, gt_bboxes = gt.split((1, 4), 2)\n",
        "        mask_gt = gt_bboxes.sum(2, keepdim=True).gt_(0)\n",
        "\n",
        "        pred_bboxes = self.box_decode(anchor_points, pred_distri)\n",
        "        assigned_targets = self.assigner(pred_scores.detach().sigmoid(),\n",
        "                                         (pred_bboxes.detach() * stride_tensor).type(gt_bboxes.dtype),\n",
        "                                         anchor_points * stride_tensor, gt_labels, gt_bboxes, mask_gt)\n",
        "        target_bboxes, target_scores, fg_mask = assigned_targets\n",
        "\n",
        "        target_scores_sum = max(target_scores.sum(), 1)\n",
        "\n",
        "        loss_cls = self.cls_loss(pred_scores, target_scores.to(data_type)).sum() / target_scores_sum  # BCE\n",
        "\n",
        "        # Box loss\n",
        "        loss_box = torch.zeros(1, device=self.device)\n",
        "        loss_dfl = torch.zeros(1, device=self.device)\n",
        "        if fg_mask.sum():\n",
        "            target_bboxes /= stride_tensor\n",
        "            loss_box, loss_dfl = self.box_loss(pred_distri,\n",
        "                                               pred_bboxes,\n",
        "                                               anchor_points,\n",
        "                                               target_bboxes,\n",
        "                                               target_scores,\n",
        "                                               target_scores_sum, fg_mask)\n",
        "\n",
        "        loss_box *= self.params['box']  # box gain\n",
        "        loss_cls *= self.params['cls']  # cls gain\n",
        "        loss_dfl *= self.params['dfl']  # dfl gain\n",
        "\n",
        "        return loss_box, loss_cls, loss_dfl"
      ],
      "metadata": {
        "id": "20Rv88gf9m8y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "D3MYfqjR9ncA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv(nn.Module):\n",
        "    def __init__(self,in_channels, out_channels,kernel_size=3,stride=1,padding=1,groups=1,activation=True):\n",
        "        super().__init__()\n",
        "        self.conv=nn.Conv2d(in_channels,out_channels,kernel_size,stride,padding,bias=False,groups=groups)\n",
        "        self.bn=nn.BatchNorm2d(out_channels,eps=0.001,momentum=0.03)\n",
        "        self.act=nn.SiLU(inplace=True) if activation else nn.Identity()\n",
        "\n",
        "    def forward(self,x):\n",
        "        return self.act(self.bn(self.conv(x)))"
      ],
      "metadata": {
        "id": "Z1-lxInT79Hd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,shortcut=True):\n",
        "        super().__init__()\n",
        "        self.conv1=Conv(in_channels,out_channels,kernel_size=3,stride=1,padding=1)\n",
        "        self.conv2=Conv(out_channels,out_channels,kernel_size=3,stride=1,padding=1)\n",
        "        self.shortcut=shortcut\n",
        "\n",
        "    def forward(self,x):\n",
        "        x_in=x # for residual connection\n",
        "        x=self.conv1(x)\n",
        "        x=self.conv2(x)\n",
        "        if self.shortcut:\n",
        "            x=x+x_in\n",
        "        return x\n",
        "\n",
        "# 2.2 C2f: Conv + bottleneck*N+ Conv\n",
        "class C2f(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels, num_bottlenecks,shortcut=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mid_channels=out_channels//2\n",
        "        self.num_bottlenecks=num_bottlenecks\n",
        "\n",
        "        self.conv1=Conv(in_channels,out_channels,kernel_size=1,stride=1,padding=0)\n",
        "\n",
        "        # sequence of bottleneck layers\n",
        "        self.m=nn.ModuleList([Bottleneck(self.mid_channels,self.mid_channels) for _ in range(num_bottlenecks)])\n",
        "\n",
        "        self.conv2=Conv((num_bottlenecks+2)*out_channels//2,out_channels,kernel_size=1,stride=1,padding=0)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x=self.conv1(x)\n",
        "\n",
        "        # split x along channel dimension\n",
        "        x1,x2=x[:,:x.shape[1]//2,:,:], x[:,x.shape[1]//2:,:,:]\n",
        "\n",
        "        # list of outputs\n",
        "        outputs=[x1,x2] # x1 is fed through the bottlenecks\n",
        "\n",
        "        for i in range(self.num_bottlenecks):\n",
        "            x1=self.m[i](x1)    # [bs,0.5c_out,w,h]\n",
        "            outputs.insert(0,x1)\n",
        "\n",
        "        outputs=torch.cat(outputs,dim=1) # [bs,0.5c_out(num_bottlenecks+2),w,h]\n",
        "        out=self.conv2(outputs)\n",
        "\n",
        "        return out\n",
        "\n",
        "# sanity check\n",
        "c2f=C2f(in_channels=64,out_channels=128,num_bottlenecks=2)\n",
        "print(f\"{sum(p.numel() for p in c2f.parameters())/1e6} million parameters\")\n",
        "\n",
        "dummy_input=torch.rand((1,64,244,244))\n",
        "dummy_input=c2f(dummy_input)\n",
        "print(\"Output shape: \", dummy_input.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toAZIPNG78-Z",
        "outputId": "0aa8da04-e9e0-4a38-906c-6827d99c41a0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.18944 million parameters\n",
            "Output shape:  torch.Size([1, 128, 244, 244])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SPPF(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,kernel_size=5):\n",
        "        #kernel_size= size of maxpool\n",
        "        super().__init__()\n",
        "        hidden_channels=in_channels//2\n",
        "        self.conv1=Conv(in_channels,hidden_channels,kernel_size=1,stride=1,padding=0)\n",
        "        # concatenate outputs of maxpool and feed to conv2\n",
        "        self.conv2=Conv(4*hidden_channels,out_channels,kernel_size=1,stride=1,padding=0)\n",
        "\n",
        "        # maxpool is applied at 3 different sacles\n",
        "        self.m=nn.MaxPool2d(kernel_size=kernel_size,stride=1,padding=kernel_size//2,dilation=1,ceil_mode=False)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x=self.conv1(x)\n",
        "\n",
        "        # apply maxpooling at diffent scales\n",
        "        y1=self.m(x)\n",
        "        y2=self.m(y1)\n",
        "        y3=self.m(y2)\n",
        "\n",
        "        # concantenate\n",
        "        y=torch.cat([x,y1,y2,y3],dim=1)\n",
        "\n",
        "        # final conv\n",
        "        y=self.conv2(y)\n",
        "\n",
        "        return y\n",
        "\n",
        "# sanity check\n",
        "sppf=SPPF(in_channels=128,out_channels=512)\n",
        "print(f\"{sum(p.numel() for p in sppf.parameters())/1e6} million parameters\")\n",
        "\n",
        "dummy_input=sppf(dummy_input)\n",
        "print(\"Output shape: \", dummy_input.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DBAh7xy78y9",
        "outputId": "3af5ad1b-da87-4f1f-ca39-c91e88c532cc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.140416 million parameters\n",
            "Output shape:  torch.Size([1, 512, 244, 244])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvfP-Sic4iZh",
        "outputId": "d3f1115e-2162-4396-f086-2c27632f93b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----Nano model -----\n",
            "1.272656 million parameters\n",
            "----Small model -----\n",
            "5.079712 million parameters\n"
          ]
        }
      ],
      "source": [
        "# backbone = DarkNet53\n",
        "\n",
        "# return d,w,r based on version\n",
        "def yolo_params(version):\n",
        "    if version=='n':\n",
        "        return 1/3,1/4,2.0\n",
        "    elif version=='s':\n",
        "        return 1/3,1/2,2.0\n",
        "    elif version=='m':\n",
        "        return 2/3,3/4,1.5\n",
        "    elif version=='l':\n",
        "        return 1.0,1.0,1.0\n",
        "    elif version=='x':\n",
        "        return 1.0,1.25,1.0\n",
        "\n",
        "class Backbone(nn.Module):\n",
        "    def __init__(self,version,in_channels=3,shortcut=True):\n",
        "        super().__init__()\n",
        "        d,w,r=yolo_params(version)\n",
        "\n",
        "        # conv layers\n",
        "        self.conv_0=Conv(in_channels,int(64*w),kernel_size=3,stride=2,padding=1)\n",
        "        self.conv_1=Conv(int(64*w),int(128*w),kernel_size=3,stride=2,padding=1)\n",
        "        self.conv_3=Conv(int(128*w),int(256*w),kernel_size=3,stride=2,padding=1)\n",
        "        self.conv_5=Conv(int(256*w),int(512*w),kernel_size=3,stride=2,padding=1)\n",
        "        self.conv_7=Conv(int(512*w),int(512*w*r),kernel_size=3,stride=2,padding=1)\n",
        "\n",
        "        # c2f layers\n",
        "        self.c2f_2=C2f(int(128*w),int(128*w),num_bottlenecks=int(3*d),shortcut=True)\n",
        "        self.c2f_4=C2f(int(256*w),int(256*w),num_bottlenecks=int(6*d),shortcut=True)\n",
        "        self.c2f_6=C2f(int(512*w),int(512*w),num_bottlenecks=int(6*d),shortcut=True)\n",
        "        self.c2f_8=C2f(int(512*w*r),int(512*w*r),num_bottlenecks=int(3*d),shortcut=True)\n",
        "\n",
        "        # sppf\n",
        "        self.sppf=SPPF(int(512*w*r),int(512*w*r))\n",
        "\n",
        "    def forward(self,x):\n",
        "        x=self.conv_0(x)\n",
        "        x=self.conv_1(x)\n",
        "\n",
        "        x=self.c2f_2(x)\n",
        "\n",
        "        x=self.conv_3(x)\n",
        "\n",
        "        out1=self.c2f_4(x) # keep for output\n",
        "\n",
        "        x=self.conv_5(out1)\n",
        "\n",
        "        out2=self.c2f_6(x) # keep for output\n",
        "\n",
        "        x=self.conv_7(out2)\n",
        "        x=self.c2f_8(x)\n",
        "        out3=self.sppf(x)\n",
        "\n",
        "        return out1,out2,out3\n",
        "\n",
        "print(\"----Nano model -----\")\n",
        "backbone_n=Backbone(version='n')\n",
        "print(f\"{sum(p.numel() for p in backbone_n.parameters())/1e6} million parameters\")\n",
        "\n",
        "print(\"----Small model -----\")\n",
        "backbone_s=Backbone(version='s')\n",
        "print(f\"{sum(p.numel() for p in backbone_s.parameters())/1e6} million parameters\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=torch.rand((1,3,640,640))\n",
        "out1,out2,out3=backbone_n(x)\n",
        "print(out1.shape)\n",
        "print(out2.shape)\n",
        "print(out3.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54afcv3K862J",
        "outputId": "d06047a7-8114-4e50-dc05-45ad908ec2f6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 64, 80, 80])\n",
            "torch.Size([1, 128, 40, 40])\n",
            "torch.Size([1, 256, 20, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# upsample = nearest-neighbor interpolation with scale_factor=2\n",
        "#            doesn't have trainable paramaters\n",
        "class Upsample(nn.Module):\n",
        "    def __init__(self,scale_factor=2,mode='nearest'):\n",
        "        super().__init__()\n",
        "        self.scale_factor=scale_factor\n",
        "        self.mode=mode\n",
        "\n",
        "    def forward(self,x):\n",
        "        return nn.functional.interpolate(x,scale_factor=self.scale_factor,mode=self.mode)\n",
        "\n",
        "# Test Upsample\n",
        "upsample = Upsample(scale_factor=2, mode='nearest')\n",
        "x = torch.rand(1, 3, 100, 100)  # Input shape: [B, C, H, W]\n",
        "output = upsample(x)\n",
        "print(f\"Upsample Test: Input {x.shape} → Output {output.shape}\")  # Should be (1,3,200,200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqC-DL3s4sa7",
        "outputId": "58604bb6-92d9-4318-8807-bc8729752c5e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upsample Test: Input torch.Size([1, 3, 100, 100]) → Output torch.Size([1, 3, 200, 200])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Neck(nn.Module):\n",
        "    def __init__(self,version):\n",
        "        super().__init__()\n",
        "        d,w,r=yolo_params(version)\n",
        "\n",
        "        self.up=Upsample() # no trainable parameters\n",
        "        self.c2f_1=C2f(in_channels=int(512*w*(1+r)), out_channels=int(512*w),num_bottlenecks=int(3*d),shortcut=False)\n",
        "        self.c2f_2=C2f(in_channels=int(768*w), out_channels=int(256*w),num_bottlenecks=int(3*d),shortcut=False)\n",
        "        self.c2f_3=C2f(in_channels=int(768*w), out_channels=int(512*w),num_bottlenecks=int(3*d),shortcut=False)\n",
        "        self.c2f_4=C2f(in_channels=int(512*w*(1+r)), out_channels=int(512*w*r),num_bottlenecks=int(3*d),shortcut=False)\n",
        "\n",
        "        self.cv_1=Conv(in_channels=int(256*w),out_channels=int(256*w),kernel_size=3,stride=2, padding=1)\n",
        "        self.cv_2=Conv(in_channels=int(512*w),out_channels=int(512*w),kernel_size=3,stride=2, padding=1)\n",
        "\n",
        "\n",
        "    def forward(self,x_res_1,x_res_2,x):\n",
        "        # x_res_1,x_res_2,x = output of backbone\n",
        "        res_1=x              # for residual connection\n",
        "\n",
        "        x=self.up(x)\n",
        "        x=torch.cat([x,x_res_2],dim=1)\n",
        "\n",
        "        res_2=self.c2f_1(x)  # for residual connection\n",
        "\n",
        "        x=self.up(res_2)\n",
        "        x=torch.cat([x,x_res_1],dim=1)\n",
        "\n",
        "        out_1=self.c2f_2(x)\n",
        "\n",
        "        x=self.cv_1(out_1)\n",
        "\n",
        "        x=torch.cat([x,res_2],dim=1)\n",
        "        out_2=self.c2f_3(x)\n",
        "\n",
        "        x=self.cv_2(out_2)\n",
        "\n",
        "        x=torch.cat([x,res_1],dim=1)\n",
        "        out_3=self.c2f_4(x)\n",
        "\n",
        "        return out_1,out_2,out_3\n",
        "\n",
        "# sanity check\n",
        "neck=Neck(version='n')\n",
        "print(f\"{sum(p.numel() for p in neck.parameters())/1e6} million parameters\")\n",
        "\n",
        "x=torch.rand((1,3,640,640))\n",
        "out1,out2,out3=Backbone(version='n')(x)\n",
        "out_1,out_2,out_3=neck(out1,out2,out3)\n",
        "print(out_1.shape)\n",
        "print(out_2.shape)\n",
        "print(out_3.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ln7H97zH6VTr",
        "outputId": "42d165b1-3c15-483f-c00b-5dc41a2b2501"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.98688 million parameters\n",
            "torch.Size([1, 64, 80, 80])\n",
            "torch.Size([1, 128, 40, 40])\n",
            "torch.Size([1, 256, 20, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DFL\n",
        "class DFL(nn.Module):\n",
        "    def __init__(self,ch=16):\n",
        "        super().__init__()\n",
        "\n",
        "        self.ch=ch\n",
        "\n",
        "        self.conv=nn.Conv2d(in_channels=ch,out_channels=1,kernel_size=1,bias=False).requires_grad_(False)\n",
        "\n",
        "        # initialize conv with [0,...,ch-1]\n",
        "        x=torch.arange(ch,dtype=torch.float).view(1,ch,1,1)\n",
        "        self.conv.weight.data[:]=torch.nn.Parameter(x) # DFL only has ch parameters\n",
        "\n",
        "    def forward(self,x):\n",
        "        # x must have num_channels = 4*ch: x=[bs,4*ch,c]\n",
        "        b,c,a=x.shape                           # c=4*ch\n",
        "        x=x.view(b,4,self.ch,a).transpose(1,2)  # [bs,ch,4,a]\n",
        "\n",
        "        # take softmax on channel dimension to get distribution probabilities\n",
        "        x=x.softmax(1)                          # [b,ch,4,a]\n",
        "        x=self.conv(x)                          # [b,1,4,a]\n",
        "        return x.view(b,4,a)                    # [b,4,a]\n",
        "\n",
        "# sanity check\n",
        "dummy_input=torch.rand((1,64,128))\n",
        "dfl=DFL()\n",
        "print(f\"{sum(p.numel() for p in dfl.parameters())} parameters\")\n",
        "\n",
        "dummy_output=dfl(dummy_input)\n",
        "print(dummy_output.shape)\n",
        "\n",
        "print(dfl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDujEMML6VR5",
        "outputId": "9c1b17da-0be5-4090-de12-01aebcbf4d27"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16 parameters\n",
            "torch.Size([1, 4, 128])\n",
            "DFL(\n",
            "  (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    def __init__(self,version,ch=16,num_classes=80):\n",
        "\n",
        "        super().__init__()\n",
        "        self.ch=ch                          # dfl channels\n",
        "        self.coordinates=self.ch*4          # number of bounding box coordinates\n",
        "        self.nc=num_classes                 # 80 for COCO\n",
        "        self.no=self.coordinates+self.nc    # number of outputs per anchor box\n",
        "\n",
        "        self.stride=torch.zeros(3)          # strides computed during build\n",
        "\n",
        "        d,w,r=yolo_params(version=version)\n",
        "\n",
        "        # for bounding boxes\n",
        "        self.box=nn.ModuleList([\n",
        "            nn.Sequential(Conv(int(256*w),self.coordinates,kernel_size=3,stride=1,padding=1),\n",
        "                          Conv(self.coordinates,self.coordinates,kernel_size=3,stride=1,padding=1),\n",
        "                          nn.Conv2d(self.coordinates,self.coordinates,kernel_size=1,stride=1)),\n",
        "\n",
        "            nn.Sequential(Conv(int(512*w),self.coordinates,kernel_size=3,stride=1,padding=1),\n",
        "                          Conv(self.coordinates,self.coordinates,kernel_size=3,stride=1,padding=1),\n",
        "                          nn.Conv2d(self.coordinates,self.coordinates,kernel_size=1,stride=1)),\n",
        "\n",
        "            nn.Sequential(Conv(int(512*w*r),self.coordinates,kernel_size=3,stride=1,padding=1),\n",
        "                          Conv(self.coordinates,self.coordinates,kernel_size=3,stride=1,padding=1),\n",
        "                          nn.Conv2d(self.coordinates,self.coordinates,kernel_size=1,stride=1))\n",
        "        ])\n",
        "\n",
        "        # for classification\n",
        "        self.cls=nn.ModuleList([\n",
        "            nn.Sequential(Conv(int(256*w),self.nc,kernel_size=3,stride=1,padding=1),\n",
        "                          Conv(self.nc,self.nc,kernel_size=3,stride=1,padding=1),\n",
        "                          nn.Conv2d(self.nc,self.nc,kernel_size=1,stride=1)),\n",
        "\n",
        "            nn.Sequential(Conv(int(512*w),self.nc,kernel_size=3,stride=1,padding=1),\n",
        "                          Conv(self.nc,self.nc,kernel_size=3,stride=1,padding=1),\n",
        "                          nn.Conv2d(self.nc,self.nc,kernel_size=1,stride=1)),\n",
        "\n",
        "            nn.Sequential(Conv(int(512*w*r),self.nc,kernel_size=3,stride=1,padding=1),\n",
        "                          Conv(self.nc,self.nc,kernel_size=3,stride=1,padding=1),\n",
        "                          nn.Conv2d(self.nc,self.nc,kernel_size=1,stride=1))\n",
        "        ])\n",
        "\n",
        "        # dfl\n",
        "        self.dfl=DFL()\n",
        "\n",
        "    def forward(self,x):\n",
        "        # x = output of Neck = list of 3 tensors with different resolution and different channel dim\n",
        "        #     x[0]=[bs, ch0, w0, h0], x[1]=[bs, ch1, w1, h1], x[2]=[bs,ch2, w2, h2]\n",
        "\n",
        "        for i in range(len(self.box)):       # detection head i\n",
        "            box=self.box[i](x[i])            # [bs,num_coordinates,w,h]\n",
        "            cls=self.cls[i](x[i])            # [bs,num_classes,w,h]\n",
        "            x[i]=torch.cat((box,cls),dim=1)  # [bs,num_coordinates+num_classes,w,h]\n",
        "\n",
        "        # in training, no dfl output\n",
        "        if self.training:\n",
        "            return x                         # [3,bs,num_coordinates+num_classes,w,h]\n",
        "\n",
        "        # in inference time, dfl produces refined bounding box coordinates\n",
        "        anchors, strides = (i.transpose(0, 1) for i in self.make_anchors(x, self.stride))\n",
        "\n",
        "        # concatenate predictions from all detection layers\n",
        "        x = torch.cat([i.view(x[0].shape[0], self.no, -1) for i in x], dim=2) #[bs, 4*self.ch + self.nc, sum_i(h[i]w[i])]\n",
        "\n",
        "        # split out predictions for box and cls\n",
        "        #           box=[bs,4×self.ch,sum_i(h[i]w[i])]\n",
        "        #           cls=[bs,self.nc,sum_i(h[i]w[i])]\n",
        "        box, cls = x.split(split_size=(4 * self.ch, self.nc), dim=1)\n",
        "\n",
        "\n",
        "        a, b = self.dfl(box).chunk(2, 1)  # a=b=[bs,2×self.ch,sum_i(h[i]w[i])]\n",
        "        a = anchors.unsqueeze(0) - a\n",
        "        b = anchors.unsqueeze(0) + b\n",
        "        box = torch.cat(tensors=((a + b) / 2, b - a), dim=1)\n",
        "\n",
        "        return torch.cat(tensors=(box * strides, cls.sigmoid()), dim=1)\n",
        "\n",
        "\n",
        "    def make_anchors(self, x, strides, offset=0.5):\n",
        "        # x= list of feature maps: x=[x[0],...,x[N-1]], in our case N= num_detection_heads=3\n",
        "        #                          each having shape [bs,ch,w,h]\n",
        "        #    each feature map x[i] gives output[i] = w*h anchor coordinates + w*h stride values\n",
        "\n",
        "        # strides = list of stride values indicating how much\n",
        "        #           the spatial resolution of the feature map is reduced compared to the original image\n",
        "\n",
        "        assert x is not None\n",
        "        anchor_tensor, stride_tensor = [], []\n",
        "        dtype, device = x[0].dtype, x[0].device\n",
        "        for i, stride in enumerate(strides):\n",
        "            _, _, h, w = x[i].shape\n",
        "            sx = torch.arange(end=w, device=device, dtype=dtype) + offset  # x coordinates of anchor centers\n",
        "            sy = torch.arange(end=h, device=device, dtype=dtype) + offset  # y coordinates of anchor centers\n",
        "            sy, sx = torch.meshgrid(sy, sx)                                # all anchor centers\n",
        "            anchor_tensor.append(torch.stack((sx, sy), -1).view(-1, 2))\n",
        "            stride_tensor.append(torch.full((h * w, 1), stride, dtype=dtype, device=device))\n",
        "        return torch.cat(anchor_tensor), torch.cat(stride_tensor)\n",
        "\n"
      ],
      "metadata": {
        "id": "c55B7zg06VQD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detect=Head(version='n')\n",
        "print(f\"{sum(p.numel() for p in detect.parameters())/1e6} million parameters\")\n",
        "\n",
        "# out_1,out_2,out_3 are output of the neck\n",
        "output=detect([out_1,out_2,out_3])\n",
        "print(output[0].shape)\n",
        "print(output[1].shape)\n",
        "print(output[2].shape)\n",
        "\n",
        "print(detect)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFxyDyCz6VN7",
        "outputId": "b61102c7-134b-4488-807b-873c2c267f29"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.897664 million parameters\n",
            "torch.Size([1, 144, 80, 80])\n",
            "torch.Size([1, 144, 40, 40])\n",
            "torch.Size([1, 144, 20, 20])\n",
            "Head(\n",
            "  (box): ModuleList(\n",
            "    (0): Sequential(\n",
            "      (0): Conv(\n",
            "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (1): Conv(\n",
            "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): Conv(\n",
            "        (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (1): Conv(\n",
            "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (2): Sequential(\n",
            "      (0): Conv(\n",
            "        (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (1): Conv(\n",
            "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (cls): ModuleList(\n",
            "    (0): Sequential(\n",
            "      (0): Conv(\n",
            "        (conv): Conv2d(64, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (1): Conv(\n",
            "        (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): Conv(\n",
            "        (conv): Conv2d(128, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (1): Conv(\n",
            "        (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (2): Sequential(\n",
            "      (0): Conv(\n",
            "        (conv): Conv2d(256, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (1): Conv(\n",
            "        (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (dfl): DFL(\n",
            "    (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyYolo(nn.Module):\n",
        "    def __init__(self,version):\n",
        "        super().__init__()\n",
        "        self.backbone=Backbone(version=version)\n",
        "        self.neck=Neck(version=version)\n",
        "        self.head=Head(version=version)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x=self.backbone(x)              # return out1,out2,out3\n",
        "        x=self.neck(x[0],x[1],x[2])     # return out_1, out_2,out_3\n",
        "        return self.head(list(x))"
      ],
      "metadata": {
        "id": "B3tRYYbd6VL8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=MyYolo(version='n')\n",
        "print(f\"{sum(p.numel() for p in model.parameters())/1e6} million parameters\")\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htbGoi876VJ2",
        "outputId": "a8414a2d-129f-4aed-975d-460af7c85128"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.1572 million parameters\n",
            "MyYolo(\n",
            "  (backbone): Backbone(\n",
            "    (conv_0): Conv(\n",
            "      (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "      (act): SiLU(inplace=True)\n",
            "    )\n",
            "    (conv_1): Conv(\n",
            "      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "      (act): SiLU(inplace=True)\n",
            "    )\n",
            "    (conv_3): Conv(\n",
            "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "      (act): SiLU(inplace=True)\n",
            "    )\n",
            "    (conv_5): Conv(\n",
            "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "      (act): SiLU(inplace=True)\n",
            "    )\n",
            "    (conv_7): Conv(\n",
            "      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "      (act): SiLU(inplace=True)\n",
            "    )\n",
            "    (c2f_2): C2f(\n",
            "      (conv1): Conv(\n",
            "        (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (m): ModuleList(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv(\n",
            "            (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (conv2): Conv(\n",
            "            (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv2): Conv(\n",
            "        (conv): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (c2f_4): C2f(\n",
            "      (conv1): Conv(\n",
            "        (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (m): ModuleList(\n",
            "        (0-1): 2 x Bottleneck(\n",
            "          (conv1): Conv(\n",
            "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (conv2): Conv(\n",
            "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv2): Conv(\n",
            "        (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (c2f_6): C2f(\n",
            "      (conv1): Conv(\n",
            "        (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (m): ModuleList(\n",
            "        (0-1): 2 x Bottleneck(\n",
            "          (conv1): Conv(\n",
            "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (conv2): Conv(\n",
            "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv2): Conv(\n",
            "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (c2f_8): C2f(\n",
            "      (conv1): Conv(\n",
            "        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (m): ModuleList(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv(\n",
            "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (conv2): Conv(\n",
            "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv2): Conv(\n",
            "        (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (sppf): SPPF(\n",
            "      (conv1): Conv(\n",
            "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (conv2): Conv(\n",
            "        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "  )\n",
            "  (neck): Neck(\n",
            "    (up): Upsample()\n",
            "    (c2f_1): C2f(\n",
            "      (conv1): Conv(\n",
            "        (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (m): ModuleList(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv(\n",
            "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (conv2): Conv(\n",
            "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv2): Conv(\n",
            "        (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (c2f_2): C2f(\n",
            "      (conv1): Conv(\n",
            "        (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (m): ModuleList(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv(\n",
            "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (conv2): Conv(\n",
            "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv2): Conv(\n",
            "        (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (c2f_3): C2f(\n",
            "      (conv1): Conv(\n",
            "        (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (m): ModuleList(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv(\n",
            "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (conv2): Conv(\n",
            "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv2): Conv(\n",
            "        (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (c2f_4): C2f(\n",
            "      (conv1): Conv(\n",
            "        (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (m): ModuleList(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv(\n",
            "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (conv2): Conv(\n",
            "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv2): Conv(\n",
            "        (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (cv_1): Conv(\n",
            "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "      (act): SiLU(inplace=True)\n",
            "    )\n",
            "    (cv_2): Conv(\n",
            "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "      (act): SiLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (head): Head(\n",
            "    (box): ModuleList(\n",
            "      (0): Sequential(\n",
            "        (0): Conv(\n",
            "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (1): Conv(\n",
            "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (1): Sequential(\n",
            "        (0): Conv(\n",
            "          (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (1): Conv(\n",
            "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (2): Sequential(\n",
            "        (0): Conv(\n",
            "          (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (1): Conv(\n",
            "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (cls): ModuleList(\n",
            "      (0): Sequential(\n",
            "        (0): Conv(\n",
            "          (conv): Conv2d(64, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (1): Conv(\n",
            "          (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (1): Sequential(\n",
            "        (0): Conv(\n",
            "          (conv): Conv2d(128, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (1): Conv(\n",
            "          (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (2): Sequential(\n",
            "        (0): Conv(\n",
            "          (conv): Conv2d(256, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (1): Conv(\n",
            "          (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (dfl): DFL(\n",
            "      (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XYg3BBln6VF7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jeB-47Ks6VD1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QxoakT4p6VBW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-cD6LcbH6U3u"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train"
      ],
      "metadata": {
        "id": "k2hEXgvQ-rX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"MXcv5t8K2GL4YetGnJ3C\")\n",
        "project = rf.workspace(\"roboflow-universe-projects\").project(\"license-plate-recognition-rxg4e\")\n",
        "version = project.version(4)\n",
        "dataset =version.download(\"YOLOv8\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTVGySzg-xsF",
        "outputId": "9d90e0b2-a308-4ab4-ad58-fa75ab07ef02"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: roboflow in /usr/local/lib/python3.11/dist-packages (1.1.58)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from roboflow) (2025.1.31)\n",
            "Requirement already satisfied: idna==3.7 in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.7)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.4.8)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.26.4)\n",
            "Requirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.10.0.84)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from roboflow) (11.1.0)\n",
            "Requirement already satisfied: pillow-heif>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.21.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.8.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.3.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (6.0.2)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.0.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (1.3.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (4.56.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (3.2.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->roboflow) (3.4.1)\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "\n",
        "import cv2\n",
        "import numpy\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.utils import data\n",
        "\n",
        "FORMATS = 'bmp', 'dng', 'jpeg', 'jpg', 'mpo', 'png', 'tif', 'tiff', 'webp'\n",
        "\n",
        "\n",
        "class YOLOV8Dataset(data.Dataset):\n",
        "    def __init__(self, filenames, input_size, params, augment):\n",
        "        self.params = params\n",
        "        self.mosaic = augment\n",
        "        self.augment = augment\n",
        "        self.input_size = input_size\n",
        "\n",
        "        # Read labels\n",
        "        labels = self.load_label(filenames)\n",
        "        self.labels = list(labels.values())\n",
        "        self.filenames = list(labels.keys())  # update\n",
        "        self.n = len(self.filenames)  # number of samples\n",
        "        self.indices = range(self.n)\n",
        "        # Albumentations (optional, only used if package is installed)\n",
        "        self.albumentations = Albumentations()\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        index = self.indices[index]\n",
        "\n",
        "        params = self.params\n",
        "        mosaic = self.mosaic and random.random() < params['mosaic']\n",
        "\n",
        "        if mosaic:\n",
        "            # Load MOSAIC\n",
        "            image, label = self.load_mosaic(index, params)\n",
        "            # MixUp augmentation\n",
        "            if random.random() < params['mix_up']:\n",
        "                index = random.choice(self.indices)\n",
        "                mix_image1, mix_label1 = image, label\n",
        "                mix_image2, mix_label2 = self.load_mosaic(index, params)\n",
        "\n",
        "                image, label = mix_up(mix_image1, mix_label1, mix_image2, mix_label2)\n",
        "        else:\n",
        "            # Load image\n",
        "            image, shape = self.load_image(index)\n",
        "            h, w = image.shape[:2]\n",
        "\n",
        "            # Resize\n",
        "            image, ratio, pad = resize(image, self.input_size, self.augment)\n",
        "\n",
        "            label = self.labels[index].copy()\n",
        "            if label.size:\n",
        "                label[:, 1:] = wh2xy(label[:, 1:], ratio[0] * w, ratio[1] * h, pad[0], pad[1])\n",
        "            if self.augment:\n",
        "                image, label = random_perspective(image, label, params)\n",
        "\n",
        "        nl = len(label)  # number of labels\n",
        "        h, w = image.shape[:2]\n",
        "        cls = label[:, 0:1]\n",
        "        box = label[:, 1:5]\n",
        "        box = xy2wh(box, w, h)\n",
        "\n",
        "        if self.augment:\n",
        "            # Albumentations\n",
        "            image, box, cls = self.albumentations(image, box, cls)\n",
        "            nl = len(box)  # update after albumentations\n",
        "            # HSV color-space\n",
        "            augment_hsv(image, params)\n",
        "            # Flip up-down\n",
        "            if random.random() < params['flip_ud']:\n",
        "                image = numpy.flipud(image)\n",
        "                if nl:\n",
        "                    box[:, 1] = 1 - box[:, 1]\n",
        "            # Flip left-right\n",
        "            if random.random() < params['flip_lr']:\n",
        "                image = numpy.fliplr(image)\n",
        "                if nl:\n",
        "                    box[:, 0] = 1 - box[:, 0]\n",
        "\n",
        "        target_cls = torch.zeros((nl, 1))\n",
        "        target_box = torch.zeros((nl, 4))\n",
        "        if nl:\n",
        "            target_cls = torch.from_numpy(cls)\n",
        "            target_box = torch.from_numpy(box)\n",
        "\n",
        "        # Convert HWC to CHW, BGR to RGB\n",
        "        sample = image.transpose((2, 0, 1))[::-1]\n",
        "        sample = numpy.ascontiguousarray(sample)\n",
        "\n",
        "        return torch.from_numpy(sample), target_cls, target_box, torch.zeros(nl)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def load_image(self, i):\n",
        "        image = cv2.imread(self.filenames[i])\n",
        "        h, w = image.shape[:2]\n",
        "        r = self.input_size / max(h, w)\n",
        "        if r != 1:\n",
        "            image = cv2.resize(image,\n",
        "                               dsize=(int(w * r), int(h * r)),\n",
        "                               interpolation=resample() if self.augment else cv2.INTER_LINEAR)\n",
        "        return image, (h, w)\n",
        "\n",
        "    def load_mosaic(self, index, params):\n",
        "        label4 = []\n",
        "        border = [-self.input_size // 2, -self.input_size // 2]\n",
        "        image4 = numpy.full((self.input_size * 2, self.input_size * 2, 3), 0, dtype=numpy.uint8)\n",
        "        y1a, y2a, x1a, x2a, y1b, y2b, x1b, x2b = (None, None, None, None, None, None, None, None)\n",
        "\n",
        "        xc = int(random.uniform(-border[0], 2 * self.input_size + border[1]))\n",
        "        yc = int(random.uniform(-border[0], 2 * self.input_size + border[1]))\n",
        "\n",
        "        indices = [index] + random.choices(self.indices, k=3)\n",
        "        random.shuffle(indices)\n",
        "\n",
        "        for i, index in enumerate(indices):\n",
        "            # Load image\n",
        "            image, _ = self.load_image(index)\n",
        "            shape = image.shape\n",
        "            if i == 0:  # top left\n",
        "                x1a = max(xc - shape[1], 0)\n",
        "                y1a = max(yc - shape[0], 0)\n",
        "                x2a = xc\n",
        "                y2a = yc\n",
        "                x1b = shape[1] - (x2a - x1a)\n",
        "                y1b = shape[0] - (y2a - y1a)\n",
        "                x2b = shape[1]\n",
        "                y2b = shape[0]\n",
        "            if i == 1:  # top right\n",
        "                x1a = xc\n",
        "                y1a = max(yc - shape[0], 0)\n",
        "                x2a = min(xc + shape[1], self.input_size * 2)\n",
        "                y2a = yc\n",
        "                x1b = 0\n",
        "                y1b = shape[0] - (y2a - y1a)\n",
        "                x2b = min(shape[1], x2a - x1a)\n",
        "                y2b = shape[0]\n",
        "            if i == 2:  # bottom left\n",
        "                x1a = max(xc - shape[1], 0)\n",
        "                y1a = yc\n",
        "                x2a = xc\n",
        "                y2a = min(self.input_size * 2, yc + shape[0])\n",
        "                x1b = shape[1] - (x2a - x1a)\n",
        "                y1b = 0\n",
        "                x2b = shape[1]\n",
        "                y2b = min(y2a - y1a, shape[0])\n",
        "            if i == 3:  # bottom right\n",
        "                x1a = xc\n",
        "                y1a = yc\n",
        "                x2a = min(xc + shape[1], self.input_size * 2)\n",
        "                y2a = min(self.input_size * 2, yc + shape[0])\n",
        "                x1b = 0\n",
        "                y1b = 0\n",
        "                x2b = min(shape[1], x2a - x1a)\n",
        "                y2b = min(y2a - y1a, shape[0])\n",
        "\n",
        "            pad_w = x1a - x1b\n",
        "            pad_h = y1a - y1b\n",
        "            image4[y1a:y2a, x1a:x2a] = image[y1b:y2b, x1b:x2b]\n",
        "\n",
        "            # Labels\n",
        "            label = self.labels[index].copy()\n",
        "            if len(label):\n",
        "                label[:, 1:] = wh2xy(label[:, 1:], shape[1], shape[0], pad_w, pad_h)\n",
        "            label4.append(label)\n",
        "\n",
        "        # Concat/clip labels\n",
        "        label4 = numpy.concatenate(label4, 0)\n",
        "        for x in label4[:, 1:]:\n",
        "            numpy.clip(x, 0, 2 * self.input_size, out=x)\n",
        "\n",
        "        # Augment\n",
        "        image4, label4 = random_perspective(image4, label4, params, border)\n",
        "\n",
        "        return image4, label4\n",
        "\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def load_label(filenames):\n",
        "        path = f'{os.path.dirname(filenames[0])}.cache'\n",
        "        if os.path.exists(path):\n",
        "            torch.serialization.add_safe_globals([numpy.ndarray])\n",
        "            return torch.load(path, weights_only=True)\n",
        "        x = {}\n",
        "        for filename in filenames:\n",
        "            try:\n",
        "                # verify images\n",
        "                with open(filename, 'rb') as f:\n",
        "                    image = Image.open(f)\n",
        "                    image.verify()  # PIL verify\n",
        "                shape = image.size  # image size\n",
        "                assert (shape[0] > 9) & (shape[1] > 9), f'image size {shape} <10 pixels'\n",
        "                assert image.format.lower() in FORMATS, f'invalid image format {image.format}'\n",
        "\n",
        "                # verify labels\n",
        "                a = f'{os.sep}images{os.sep}'\n",
        "                b = f'{os.sep}labels{os.sep}'\n",
        "                if os.path.isfile(b.join(filename.rsplit(a, 1)).rsplit('.', 1)[0] + '.txt'):\n",
        "                    with open(b.join(filename.rsplit(a, 1)).rsplit('.', 1)[0] + '.txt') as f:\n",
        "                        label = [x.split() for x in f.read().strip().splitlines() if len(x)]\n",
        "                        label = numpy.array(label, dtype=numpy.float32)\n",
        "                    nl = len(label)\n",
        "                    if nl:\n",
        "                        assert (label >= 0).all()\n",
        "                        assert label.shape[1] == 5\n",
        "                        assert (label[:, 1:] <= 1).all()\n",
        "                        _, i = numpy.unique(label, axis=0, return_index=True)\n",
        "                        if len(i) < nl:  # duplicate row check\n",
        "                            label = label[i]  # remove duplicates\n",
        "                    else:\n",
        "                        label = numpy.zeros((0, 5), dtype=numpy.float32)\n",
        "                else:\n",
        "                    label = numpy.zeros((0, 5), dtype=numpy.float32)\n",
        "            except FileNotFoundError:\n",
        "                label = numpy.zeros((0, 5), dtype=numpy.float32)\n",
        "            except AssertionError:\n",
        "                continue\n",
        "            x[filename] = label\n",
        "        torch.save(x, path)\n",
        "        return x\n",
        "\n",
        "\n",
        "def wh2xy(x, w=640, h=640, pad_w=0, pad_h=0):\n",
        "    # Convert nx4 boxes\n",
        "    # from [x, y, w, h] normalized to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
        "    y = numpy.copy(x)\n",
        "    y[:, 0] = w * (x[:, 0] - x[:, 2] / 2) + pad_w  # top left x\n",
        "    y[:, 1] = h * (x[:, 1] - x[:, 3] / 2) + pad_h  # top left y\n",
        "    y[:, 2] = w * (x[:, 0] + x[:, 2] / 2) + pad_w  # bottom right x\n",
        "    y[:, 3] = h * (x[:, 1] + x[:, 3] / 2) + pad_h  # bottom right y\n",
        "    return y\n",
        "\n",
        "\n",
        "def xy2wh(x, w, h):\n",
        "    # warning: inplace clip\n",
        "    x[:, [0, 2]] = x[:, [0, 2]].clip(0, w - 1E-3)  # x1, x2\n",
        "    x[:, [1, 3]] = x[:, [1, 3]].clip(0, h - 1E-3)  # y1, y2\n",
        "\n",
        "    # Convert nx4 boxes\n",
        "    # from [x1, y1, x2, y2] to [x, y, w, h] normalized where xy1=top-left, xy2=bottom-right\n",
        "    y = numpy.copy(x)\n",
        "    y[:, 0] = ((x[:, 0] + x[:, 2]) / 2) / w  # x center\n",
        "    y[:, 1] = ((x[:, 1] + x[:, 3]) / 2) / h  # y center\n",
        "    y[:, 2] = (x[:, 2] - x[:, 0]) / w  # width\n",
        "    y[:, 3] = (x[:, 3] - x[:, 1]) / h  # height\n",
        "    return y\n",
        "\n",
        "\n",
        "def resample():\n",
        "    choices = (cv2.INTER_AREA,\n",
        "               cv2.INTER_CUBIC,\n",
        "               cv2.INTER_LINEAR,\n",
        "               cv2.INTER_NEAREST,\n",
        "               cv2.INTER_LANCZOS4)\n",
        "    return random.choice(seq=choices)\n",
        "\n",
        "\n",
        "def augment_hsv(image, params):\n",
        "    # HSV color-space augmentation\n",
        "    h = params['hsv_h']\n",
        "    s = params['hsv_s']\n",
        "    v = params['hsv_v']\n",
        "\n",
        "    r = numpy.random.uniform(-1, 1, 3) * [h, s, v] + 1\n",
        "    h, s, v = cv2.split(cv2.cvtColor(image, cv2.COLOR_BGR2HSV))\n",
        "\n",
        "    x = numpy.arange(0, 256, dtype=r.dtype)\n",
        "    lut_h = ((x * r[0]) % 180).astype('uint8')\n",
        "    lut_s = numpy.clip(x * r[1], 0, 255).astype('uint8')\n",
        "    lut_v = numpy.clip(x * r[2], 0, 255).astype('uint8')\n",
        "\n",
        "    hsv = cv2.merge((cv2.LUT(h, lut_h), cv2.LUT(s, lut_s), cv2.LUT(v, lut_v)))\n",
        "    cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR, dst=image)  # no return needed\n",
        "\n",
        "\n",
        "def resize(image, input_size, augment):\n",
        "    # Resize and pad image while meeting stride-multiple constraints\n",
        "    shape = image.shape[:2]  # current shape [height, width]\n",
        "\n",
        "    # Scale ratio (new / old)\n",
        "    r = min(input_size / shape[0], input_size / shape[1])\n",
        "    if not augment:  # only scale down, do not scale up (for better val mAP)\n",
        "        r = min(r, 1.0)\n",
        "\n",
        "    # Compute padding\n",
        "    pad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "    w = (input_size - pad[0]) / 2\n",
        "    h = (input_size - pad[1]) / 2\n",
        "\n",
        "    if shape[::-1] != pad:  # resize\n",
        "        image = cv2.resize(image,\n",
        "                           dsize=pad,\n",
        "                           interpolation=resample() if augment else cv2.INTER_LINEAR)\n",
        "    top, bottom = int(round(h - 0.1)), int(round(h + 0.1))\n",
        "    left, right = int(round(w - 0.1)), int(round(w + 0.1))\n",
        "    image = cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT)  # add border\n",
        "    return image, (r, r), (w, h)\n",
        "\n",
        "\n",
        "def candidates(box1, box2):\n",
        "    # box1(4,n), box2(4,n)\n",
        "    w1, h1 = box1[2] - box1[0], box1[3] - box1[1]\n",
        "    w2, h2 = box2[2] - box2[0], box2[3] - box2[1]\n",
        "    aspect_ratio = numpy.maximum(w2 / (h2 + 1e-16), h2 / (w2 + 1e-16))  # aspect ratio\n",
        "    return (w2 > 2) & (h2 > 2) & (w2 * h2 / (w1 * h1 + 1e-16) > 0.1) & (aspect_ratio < 100)\n",
        "\n",
        "\n",
        "def random_perspective(image, label, params, border=(0, 0)):\n",
        "    h = image.shape[0] + border[0] * 2\n",
        "    w = image.shape[1] + border[1] * 2\n",
        "\n",
        "    # Center\n",
        "    center = numpy.eye(3)\n",
        "    center[0, 2] = -image.shape[1] / 2  # x translation (pixels)\n",
        "    center[1, 2] = -image.shape[0] / 2  # y translation (pixels)\n",
        "\n",
        "    # Perspective\n",
        "    perspective = numpy.eye(3)\n",
        "\n",
        "    # Rotation and Scale\n",
        "    rotate = numpy.eye(3)\n",
        "    a = random.uniform(-params['degrees'], params['degrees'])\n",
        "    s = random.uniform(1 - params['scale'], 1 + params['scale'])\n",
        "    rotate[:2] = cv2.getRotationMatrix2D(angle=a, center=(0, 0), scale=s)\n",
        "\n",
        "    # Shear\n",
        "    shear = numpy.eye(3)\n",
        "    shear[0, 1] = math.tan(random.uniform(-params['shear'], params['shear']) * math.pi / 180)\n",
        "    shear[1, 0] = math.tan(random.uniform(-params['shear'], params['shear']) * math.pi / 180)\n",
        "\n",
        "    # Translation\n",
        "    translate = numpy.eye(3)\n",
        "    translate[0, 2] = random.uniform(0.5 - params['translate'], 0.5 + params['translate']) * w\n",
        "    translate[1, 2] = random.uniform(0.5 - params['translate'], 0.5 + params['translate']) * h\n",
        "\n",
        "    # Combined rotation matrix, order of operations (right to left) is IMPORTANT\n",
        "    matrix = translate @ shear @ rotate @ perspective @ center\n",
        "    if (border[0] != 0) or (border[1] != 0) or (matrix != numpy.eye(3)).any():  # image changed\n",
        "        image = cv2.warpAffine(image, matrix[:2], dsize=(w, h), borderValue=(0, 0, 0))\n",
        "\n",
        "    # Transform label coordinates\n",
        "    n = len(label)\n",
        "    if n:\n",
        "        xy = numpy.ones((n * 4, 3))\n",
        "        xy[:, :2] = label[:, [1, 2, 3, 4, 1, 4, 3, 2]].reshape(n * 4, 2)  # x1y1, x2y2, x1y2, x2y1\n",
        "        xy = xy @ matrix.T  # transform\n",
        "        xy = xy[:, :2].reshape(n, 8)  # perspective rescale or affine\n",
        "\n",
        "        # create new boxes\n",
        "        x = xy[:, [0, 2, 4, 6]]\n",
        "        y = xy[:, [1, 3, 5, 7]]\n",
        "        box = numpy.concatenate((x.min(1), y.min(1), x.max(1), y.max(1))).reshape(4, n).T\n",
        "\n",
        "        # clip\n",
        "        box[:, [0, 2]] = box[:, [0, 2]].clip(0, w)\n",
        "        box[:, [1, 3]] = box[:, [1, 3]].clip(0, h)\n",
        "        # filter candidates\n",
        "        indices = candidates(box1=label[:, 1:5].T * s, box2=box.T)\n",
        "\n",
        "        label = label[indices]\n",
        "        label[:, 1:5] = box[indices]\n",
        "\n",
        "    return image, label\n",
        "\n",
        "\n",
        "def mix_up(image1, label1, image2, label2):\n",
        "    # Applies MixUp augmentation https://arxiv.org/pdf/1710.09412.pdf\n",
        "    alpha = numpy.random.beta(a=32.0, b=32.0)  # mix-up ratio, alpha=beta=32.0\n",
        "    image = (image1 * alpha + image2 * (1 - alpha)).astype(numpy.uint8)\n",
        "    label = numpy.concatenate((label1, label2), 0)\n",
        "    return image, label\n",
        "\n",
        "\n",
        "class Albumentations:\n",
        "    def __init__(self):\n",
        "        self.transform = None\n",
        "        try:\n",
        "            import albumentations\n",
        "\n",
        "            transforms = [albumentations.Blur(p=0.01),\n",
        "                          albumentations.CLAHE(p=0.01),\n",
        "                          albumentations.ToGray(p=0.01),\n",
        "                          albumentations.MedianBlur(p=0.01)]\n",
        "            self.transform = albumentations.Compose(transforms,\n",
        "                                                    albumentations.BboxParams('yolo', ['class_labels']))\n",
        "\n",
        "        except ImportError:  # package not installed, skip\n",
        "            pass\n",
        "\n",
        "    def __call__(self, image, box, cls):\n",
        "        if self.transform:\n",
        "            x = self.transform(image=image,\n",
        "                               bboxes=box,\n",
        "                               class_labels=cls)\n",
        "            image = x['image']\n",
        "            box = numpy.array(x['bboxes'])\n",
        "            cls = numpy.array(x['class_labels'])\n",
        "        return image, box, cls\n",
        "\n"
      ],
      "metadata": {
        "id": "Pmjw6BEx_W8u"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataset_root = dataset.location\n",
        "split = \"train\"  # or \"val\"\n",
        "image_dir = os.path.join(dataset_root, split, \"images\")\n",
        "image_files = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith((\".jpg\", \".png\"))]\n",
        "\n",
        "# Step 2: Initialize Dataset\n",
        "input_size = 640  # YOLOv8 input resoluti\n",
        "params = {\n",
        "    \"min_lr\": 0.0001,             # Initial learning rate\n",
        "    \"max_lr\": 0.01,               # Maximum learning rate\n",
        "    \"momentum\": 0.937,            # SGD momentum/Adam beta1\n",
        "    \"weight_decay\": 0.0005,        # Optimizer weight decay\n",
        "    \"warmup_epochs\": 3.0,         # Warmup epochs\n",
        "\n",
        "    # Loss Function Parameters\n",
        "    \"box\": 7.5,                   # Box loss gain\n",
        "    \"cls\": 0.5,                   # Classification loss gain\n",
        "    \"dfl\": 1.5,                   # Distribution Focal Loss (DFL) gain\n",
        "\n",
        "    # Data Augmentation Parameters\n",
        "    \"hsv_h\": 0.015,               # Image HSV-Hue augmentation (fraction)\n",
        "    \"hsv_s\": 0.7,                 # Image HSV-Saturation augmentation (fraction)\n",
        "    \"hsv_v\": 0.4,                 # Image HSV-Value augmentation (fraction)\n",
        "    \"degrees\": 0.0,               # Image rotation (+/- degrees)\n",
        "    \"translate\": 0.1,             # Image translation (+/- fraction)\n",
        "    \"scale\": 0.5,                 # Image scale (+/- gain)\n",
        "    \"shear\": 0.0,                 # Image shear (+/- degrees)\n",
        "    \"flip_ud\": 0.0,               # Image flip up-down (probability)\n",
        "    \"flip_lr\": 0.5,               # Image flip left-right (probability)\n",
        "    \"mosaic\": 1.0,                # Image mosaic (probability)\n",
        "    \"mix_up\": 0.0,                 # Image mix-up (probability)\n",
        "\n",
        "\n",
        "    \"names\": {0: \"license\"},  # Single-class dataset\n",
        "}\n",
        "\n",
        "train_data=YOLOV8Dataset(image_files, input_size,params,augment=True)"
      ],
      "metadata": {
        "id": "S7WkxJEi_hOB"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwnpLeSOSHf_",
        "outputId": "131fb916-ea7b-484b-e500-a8cdabe02323"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[33, 39, 48,  ...,  0,  0,  0],\n",
              "          [32, 37, 44,  ...,  0,  0,  0],\n",
              "          [31, 34, 40,  ...,  0,  0,  0],\n",
              "          ...,\n",
              "          [76, 77, 77,  ...,  0,  0,  0],\n",
              "          [76, 76, 77,  ...,  0,  0,  0],\n",
              "          [76, 77, 77,  ...,  0,  0,  0]],\n",
              " \n",
              "         [[53, 63, 74,  ...,  0,  0,  0],\n",
              "          [53, 60, 71,  ...,  0,  0,  0],\n",
              "          [52, 58, 67,  ...,  0,  0,  0],\n",
              "          ...,\n",
              "          [80, 81, 81,  ...,  0,  0,  0],\n",
              "          [80, 80, 81,  ...,  0,  0,  0],\n",
              "          [80, 81, 81,  ...,  0,  0,  0]],\n",
              " \n",
              "         [[65, 75, 87,  ...,  0,  0,  0],\n",
              "          [65, 73, 85,  ...,  0,  0,  0],\n",
              "          [64, 70, 81,  ...,  0,  0,  0],\n",
              "          ...,\n",
              "          [77, 78, 78,  ...,  0,  0,  0],\n",
              "          [77, 77, 78,  ...,  0,  0,  0],\n",
              "          [77, 78, 78,  ...,  0,  0,  0]]], dtype=torch.uint8),\n",
              " tensor([], size=(0, 1)),\n",
              " tensor([], size=(0, 4)),\n",
              " tensor([]))"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4eqA6r9eSaaI"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    samples, cls, box, indices = zip(*batch)\n",
        "\n",
        "    cls = torch.cat(cls, dim=0)\n",
        "    box = torch.cat(box, dim=0)\n",
        "\n",
        "    new_indices = list(indices)\n",
        "    for i in range(len(indices)):\n",
        "        new_indices[i] += i\n",
        "    indices = torch.cat(new_indices, dim=0)\n",
        "\n",
        "    targets = {'cls': cls,\n",
        "                   'box': box,\n",
        "                   'idx': indices}\n",
        "    return torch.stack(samples, dim=0), targets\n",
        "\n",
        "def collate_fn(batch):\n",
        "    samples, cls, box, indices = zip(*batch)\n",
        "\n",
        "    cls = [c.view(-1, 1) if c.dim() == 1 else c for c in cls]  # Ensure cls has two dimensions\n",
        "    box = [b.view(-1, 4) if b.dim() == 1 else b for b in box]  # Ensure box has two dimensions\n",
        "\n",
        "    cls = torch.cat(cls, dim=0)\n",
        "    box = torch.cat(box, dim=0)\n",
        "\n",
        "    new_indices = list(indices)\n",
        "    for i in range(len(indices)):\n",
        "        new_indices[i] += i\n",
        "    indices = torch.cat(new_indices, dim=0)\n",
        "\n",
        "    targets = {'cls': cls,\n",
        "               'box': box,\n",
        "               'idx': indices}\n",
        "    return torch.stack(samples, dim=0), targets\n",
        "\n",
        "\n",
        "train_loader = data.DataLoader(train_data, batch_size=16, num_workers=0, pin_memory=True, collate_fn=collate_fn)\n",
        "print(f\"Train_loader : {len(train_loader)} batches\")\n",
        "\n",
        "\n",
        "batch=next(iter(train_loader))\n",
        "print(\"All keys in batch      : \", batch[1].keys())\n",
        "print(f\"Input batch shape      : \", batch[0].shape)\n",
        "print(f\"Classification scores  : {batch[1]['cls'].shape}\")\n",
        "print(f\"Box coordinates        : {batch[1]['box'].shape}\")\n",
        "print(f\"Index identifier (which score belongs to which image): {batch[1]['idx'].shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "we-MQsWFNR29",
        "outputId": "a666669a-9bd1-4d70-8773-87e6621ee71a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_loader : 1324 batches\n",
            "All keys in batch      :  dict_keys(['cls', 'box', 'idx'])\n",
            "Input batch shape      :  torch.Size([16, 3, 640, 640])\n",
            "Classification scores  : torch.Size([23, 1])\n",
            "Box coordinates        : torch.Size([23, 4])\n",
            "Index identifier (which score belongs to which image): torch.Size([23])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "# model, loss and optimizer\n",
        "model=MyYolo(version='n')\n",
        "print(f\"{sum(p.numel() for p in model.parameters())/1e6} million parameters\")\n",
        "\n",
        "criterion=ComputeLoss(model, params)\n",
        "optimizer=torch.optim.AdamW(model.parameters(), lr=0.5)\n",
        "\n",
        "num_epochs=5\n",
        "\n",
        "imgs,targets=batch[0],batch[1]\n",
        "imgs=imgs.float()\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    outputs=model(imgs)\n",
        "    loss=sum(criterion(outputs,targets)) # cls_loss+box_loss+dfl_loss\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch : {epoch} | loss : {loss.item()}\")\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4XQt7mOUXVQ",
        "outputId": "2bb3f7e9-1f4a-4111-8f6e-0bd16476761b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.1572 million parameters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 0 | loss : 7626365.0\n",
            "Epoch : 1 | loss : 1489456.625\n",
            "Epoch : 2 | loss : 1.7169008255004883\n",
            "Epoch : 3 | loss : 0.0\n",
            "Epoch : 4 | loss : 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Set device (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize Model, Loss Function, and Optimizer\n",
        "model = MyYolo(version='n').to(device)\n",
        "print(f\"{sum(p.numel() for p in model.parameters()) / 1e6} million parameters\")\n",
        "\n",
        "criterion = ComputeLoss(model ,params)  # Ensure params are correctly initialized\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:  # Loop through the entire dataset\n",
        "        imgs, targets = batch[0], batch[1]\n",
        "\n",
        "        # Move data to GPU (if available)\n",
        "        imgs = imgs.float().to(device)\n",
        "        targets = {k: v.to(device) for k, v in targets.items()}\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(imgs)\n",
        "        loss = sum(criterion(outputs, targets))  # Compute total loss (cls_loss + box_loss + dfl_loss)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch: {epoch+1}/{num_epochs} | Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"Training Complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8k8fMxvYX1TA",
        "outputId": "bbc7b0ee-9f28-445f-92ca-2da0fc39d755"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.1572 million parameters\n",
            "Epoch: 1/5 | Avg Loss: 333568.3286\n",
            "Epoch: 2/5 | Avg Loss: 5092.3283\n",
            "Epoch: 3/5 | Avg Loss: 1409.4721\n"
          ]
        }
      ]
    }
  ]
}